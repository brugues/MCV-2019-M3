{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students**: Sergio Casas Pastor, Sanket Biswas and Josep Brugués i Pujolràs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "   import cPickle as pickle # Python 2\n",
    "except:\n",
    "   import pickle # Python 3\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import Normalizer, PowerTransformer, MinMaxScaler\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first read the train and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3b27b1f58907>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtrain_images_filenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_images_filenames.dat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtest_images_filenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_images_filenames.dat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3b27b1f58907>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_labels.dat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain_images_filenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_images_filenames.dat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtest_images_filenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_images_filenames.dat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_labels.dat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: the STRING opcode argument must be quoted"
     ],
     "ename": "UnpicklingError",
     "evalue": "the STRING opcode argument must be quoted",
     "output_type": "error"
    }
   ],
   "source": [
    "try:\n",
    "    train_images_filenames = pickle.load(open('train_images_filenames.dat','r'))\n",
    "    test_images_filenames = pickle.load(open('test_images_filenames.dat','r'))\n",
    "    train_labels = pickle.load(open('train_labels.dat','r'))\n",
    "    test_labels = pickle.load(open('test_labels.dat','r'))\n",
    "except:\n",
    "    train_images_filenames = pickle.load(open('train_images_filenames.dat','rb'))\n",
    "    test_images_filenames = pickle.load(open('test_images_filenames.dat','rb'))\n",
    "    train_labels = pickle.load(open('train_labels.dat','rb'))\n",
    "    test_labels = pickle.load(open('test_labels.dat','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_images_filenames[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print (set(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We create a SIFT object detector and descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SIFTdetector = cv2.xfeatures2d.SIFT_create(nfeatures=900)\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We create a Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "norm = \"minmax\" #Valid norms: l1, l2, max, power, minmax\n",
    "\n",
    "if norm == \"power\":\n",
    "    transformer = PowerTransformer() # Defaults to ’yeo-johnson’ method \n",
    "                                                 # (Not working with scikit 0.20.4 because of a bug)\n",
    "elif norm == \"minmax\":\n",
    "    transformer = MinMaxScaler() # Defaults to 0-1 range\n",
    "else: # l1, l2 or max\n",
    "    try:\n",
    "        transformer = Normalizer(norm=norm)\n",
    "    except:\n",
    "        print(\"Invalid norm. Using L2 instead\")\n",
    "        transformer = Normalizer(norm=\"l2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We compute different spatial pyramid level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "level = 1\n",
    "level = 2**(level-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We compute the SIFT descriptors for the all the train images and subsequently build a numpy array with all the descriptors stacked together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Train_descriptors = []\n",
    "Train_label_per_descriptor = []\n",
    "\n",
    "for filename,labels in zip(train_images_filenames,train_labels):\n",
    "    ima=cv2.imread(filename)\n",
    "    \n",
    "    # Resize image\n",
    "    scale_percent = 100 # percent of original size\n",
    "    width = int(ima.shape[1] * scale_percent / 100)\n",
    "    height = int(ima.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    ima = cv2.resize(ima, dim, interpolation = cv2.INTER_AREA) \n",
    "    \n",
    "    gray=cv2.cvtColor(ima,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    step = 8 # Separation of pixels between keypoints\n",
    "    descriptors = []\n",
    "\n",
    "    # Compute spatial pyramids\n",
    "    height_block = int(np.ceil(ima.shape[0] / level))  # Number of height pixels for sub-image\n",
    "    width_block = int(np.ceil(ima.shape[1] / level))   # Number of width pixels for sub-image\n",
    "    \n",
    "    for height in range(0, ima.shape[0], height_block):\n",
    "        for width in range(0, ima.shape[1], width_block):\n",
    "            block = gray[height:height + height_block, width:width + width_block]\n",
    "            keypoints_block = []\n",
    "        \n",
    "            for i in range(int(step/2), block.shape[0], step):\n",
    "                for j in range(int(step/2), block.shape[1], step):             \n",
    "                    keypoints_block.append(cv2.KeyPoint(i, j, step))\n",
    "     \n",
    "            _, descriptors_block = SIFTdetector.compute(block,keypoints_block)\n",
    "            descriptors.extend(descriptors_block)\n",
    "    \n",
    "    Train_descriptors.append(descriptors)\n",
    "    Train_label_per_descriptor.append(labels)\n",
    "\n",
    "Train_descriptors = np.array(Train_descriptors)\n",
    "nsamples, nx, ny = Train_descriptors.shape\n",
    "Train_descriptors = Train_descriptors.reshape((nsamples,nx*ny))\n",
    "#nsamples, nx = Train_descriptors.shape\n",
    "Train_descriptors_scaled = transformer.fit_transform(Train_descriptors)\n",
    "Train_descriptors_scaled = Train_descriptors_scaled.reshape((nsamples, nx, ny))\n",
    "D=np.vstack(Train_descriptors_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SVM</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Adapt GT\n",
    "gt_classes = np.zeros((len(train_labels)))\n",
    "for i in range (len(train_labels)):\n",
    "    if train_labels[i] == 'Opencountry':\n",
    "        gt_classes[i] = 0\n",
    "    elif train_labels[i] == 'coast':\n",
    "        gt_classes[i] = 1\n",
    "    elif train_labels[i] == 'forest':\n",
    "        gt_classes[i] = 2\n",
    "    elif train_labels[i] == 'highway':\n",
    "        gt_classes[i] = 3\n",
    "    elif train_labels[i] == 'inside_city':\n",
    "        gt_classes[i] = 4\n",
    "    elif train_labels[i] == 'mountain':\n",
    "        gt_classes[i] = 5\n",
    "    elif train_labels[i] == 'street':\n",
    "        gt_classes[i] = 6\n",
    "    elif train_labels[i] == 'tallbuilding':\n",
    "        gt_classes[i] = 7\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Standard SCaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(Train_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(kernel='linear')\n",
    "svm_clf.fit(X_train_minmax, gt_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "np.shape(X_train_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Test_descriptors = []\n",
    "for image in range(len(test_images_filenames)):\n",
    "    filename=test_images_filenames[image]\n",
    "    ima=cv2.imread(filename)\n",
    "    \n",
    "    # Resize image\n",
    "    width = int(ima.shape[1] * scale_percent / 100)\n",
    "    height = int(ima.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    ima = cv2.resize(ima, dim, interpolation = cv2.INTER_AREA) \n",
    "    \n",
    "    gray=cv2.cvtColor(ima,cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "    descriptors = []\n",
    "\n",
    "    # Compute spatial pyramids\n",
    "    height_block = int(np.ceil(ima.shape[0] / level))  # Number of height pixels for sub-image\n",
    "    width_block = int(np.ceil(ima.shape[1] / level))    # Number of width pixels for sub-image\n",
    "    for height in range(0, ima.shape[0], height_block):\n",
    "        for width in range(0, ima.shape[1], width_block):\n",
    "            block = gray[height:height + height_block, width:width + width_block]\n",
    "            keypoints_block = []\n",
    "    \n",
    "            for i in range(int(step/2), block.shape[0], step):\n",
    "                for j in range(int(step/2), block.shape[1], step):             \n",
    "                    keypoints_block.append(cv2.KeyPoint(i, j, step))\n",
    "            \n",
    "            _, descriptors_block = SIFTdetector.compute(block,keypoints_block)   \n",
    "            descriptors.extend(descriptors_block)\n",
    "    \n",
    "    Test_descriptors.append(descriptors)\n",
    "    \n",
    "Test_descriptors = np.array(Test_descriptors)\n",
    "nsamples, nx, ny = Test_descriptors.shape\n",
    "Test_descriptors = Test_descriptors.reshape((nsamples, nx*ny))\n",
    "Test_descriptors_scaled = min_max_scaler.fit_transform(Test_descriptors)\n",
    "Test_descriptors_scaled = Test_descriptors_scaled.reshape((nsamples, nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "SVM_pred = svm_clf.predict(Test_descriptors_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# create test labels classes\n",
    "gt_classes_test = np.zeros((len(test_labels)))\n",
    "for i in range (len(test_labels)):\n",
    "    if test_labels[i] == 'Opencountry':\n",
    "        gt_classes_test[i] = 0\n",
    "    elif test_labels[i] == 'coast':\n",
    "        gt_classes_test[i] = 1\n",
    "    elif test_labels[i] == 'forest':\n",
    "        gt_classes_test[i] = 2\n",
    "    elif test_labels[i] == 'highway':\n",
    "        gt_classes_test[i] = 3\n",
    "    elif test_labels[i] == 'inside_city':\n",
    "        gt_classes_test[i] = 4\n",
    "    elif test_labels[i] == 'mountain':\n",
    "        gt_classes_test[i] = 5\n",
    "    elif test_labels[i] == 'street':\n",
    "        gt_classes_test[i] = 6\n",
    "    elif test_labels[i] == 'tallbuilding':\n",
    "        gt_classes_test[i] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(gt_classes_test, SVM_pred, normalize=True)\n",
    "print (\"Accuracy of SVM-linear: %.4f\" %(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# SVM with RBF\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf_rbf = SVC(kernel='rbf')\n",
    "svm_clf_rbf.fit(X_train_minmax, gt_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "SVM_pred_rbf = svm_clf_rbf.predict(Test_descriptors_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(gt_classes_test, SVM_pred_rbf, normalize=True)\n",
    "print (\"Accuracy of SVM-rbf: %.4f\" %(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# HISTOGRAM INTERSECTION KERNEL\n",
    "\n",
    "def histogramIntersection(M, N):\n",
    "    N = np.transpose(N)\n",
    "    K_int = np.zeros((M.shape[0], N.shape[1]), dtype=float)\n",
    "\n",
    "    for y in range(M.shape[0]):\n",
    "        for x in range(N.shape[1]):\n",
    "            K_int[y, x] = np.sum(np.minimum(M[y, :], N[:, x]))\n",
    "\n",
    "    return K_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "kernelMatrix = histogramIntersection(X_train_minmax, X_train_minmax)\n",
    "clf_histogram = svm.SVC(kernel='precomputed',C=10, gamma=.002)\n",
    "clf_histogram.fit(kernelMatrix, gt_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "predictMatrix = histogramIntersection(Test_descriptors_scaled, X_train_minmax)\n",
    "SVMpredictions = clf_histogram.predict(predictMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "acc = accuracy_score(gt_classes_test, SVMpredictions, normalize=True)\n",
    "print (\"Accuracy of SVM-Histogram: %.4f\" %(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we find the best parameters for SVC-RBF using Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "C_range = 10. ** np.arange(-3, 8)\n",
    "gamma_range = 10. ** np.arange(-5, 4)\n",
    "\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "\n",
    "grid = GridSearchCV(SVC(kernel=\"rbf\"), param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train_minmax, gt_classes)\n",
    "try:\n",
    "    meanslda = 100 * clflda.cv_results_['mean_test_score']\n",
    "    print (\"Maximum accuracy for validation set is\", max(meanslda), \"with n_neighbors =\", clflda.best_params_['n_neighbors'], \"and metric =\",  clflda.best_params_['metric']) \n",
    "except:\n",
    "    print(\"Error in print\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "result = grid.predict(Test_descriptors_scaled, gt_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could not complete this running because it took too much time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>KMEANS ORIGINAL CODE</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now compute a k-means clustering on the descriptor space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k = 170\n",
    "codebook = MiniBatchKMeans(n_clusters=k, verbose=False, batch_size=k * 20,compute_labels=False,reassignment_ratio=10**-4,random_state=42)\n",
    "codebook.fit(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And, for each train image, we project each keypoint descriptor to its closest visual word. We represent each of the images with the frequency of each visual word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visual_words=np.zeros((len(Train_descriptors_scaled),k),dtype=np.float32)\n",
    "for i in range(len(Train_descriptors)):\n",
    "    words=codebook.predict(Train_descriptors_scaled[i])\n",
    "    visual_words[i,:]=np.bincount(words,minlength=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We build a k-nn classifier and train it with the train descriptors. We use GridSearchCV to do Cross-Validation and find the best parameters of the k-nn classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find best parameters\n",
    "parameters = {'n_neighbors':[1,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100], \\\n",
    "              'metric':['euclidean','manhattan','chebyshev','minkowski']}\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "clf = GridSearchCV(knn, parameters, cv=5) # StratifiedKFold n_splits = 5\n",
    "clf.fit(visual_words, train_labels) \n",
    "\n",
    "print(\"Parameters set found on development set:\")\n",
    "means = 100 * clf.cv_results_['mean_test_score']\n",
    "\n",
    "# Train again with best parameters\n",
    "knn_best = KNeighborsClassifier(n_neighbors= clf.best_params_['n_neighbors'],n_jobs=-1,metric=clf.best_params_['metric'])\n",
    "knn_best.fit(visual_words, train_labels)\n",
    "\n",
    "plt.scatter(parameters['n_neighbors'], means[0:21])\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Accuracy (%) for euclidean distance\")\n",
    "plt.ylim((40,100))\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(parameters['n_neighbors'], means[21:42])\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Accuracy (%) for manhattan distance\")\n",
    "plt.ylim((40,100))\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(parameters['n_neighbors'], means[42:63])\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Accuracy (%) for chebyshev distance\")\n",
    "plt.ylim((40,100))\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(parameters['n_neighbors'], means[63:84])\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Accuracy (%) for minkowski distance\")\n",
    "plt.ylim((40,100))\n",
    "plt.show()\n",
    "\n",
    "print (\"Maximum accuracy for validation set is\", max(means), \"with n_neighbors =\", clf.best_params_['n_neighbors'], \"and metric =\",  clf.best_params_['metric']) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up computing the test descriptors and compute the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "visual_words_test=np.zeros((len(test_images_filenames),k),dtype=np.float32)\n",
    "Test_descriptors = []\n",
    "for image in range(len(test_images_filenames)):\n",
    "    filename=test_images_filenames[image]\n",
    "    ima=cv2.imread(filename)\n",
    "    \n",
    "    # Resize image\n",
    "    width = int(ima.shape[1] * scale_percent / 100)\n",
    "    height = int(ima.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    ima = cv2.resize(ima, dim, interpolation = cv2.INTER_AREA) \n",
    "    \n",
    "    gray=cv2.cvtColor(ima,cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "    descriptors = []\n",
    "\n",
    "    # Compute spatial pyramids\n",
    "    height_block = int(np.ceil(ima.shape[0] / level))  # Number of height pixels for sub-image\n",
    "    width_block = int(np.ceil(ima.shape[1] / level))    # Number of width pixels for sub-image\n",
    "    for height in range(0, ima.shape[0], height_block):\n",
    "        for width in range(0, ima.shape[1], width_block):\n",
    "            block = gray[height:height + height_block, width:width + width_block]\n",
    "            keypoints_block = []\n",
    "    \n",
    "            for i in range(int(step/2), block.shape[0], step):\n",
    "                for j in range(int(step/2), block.shape[1], step):             \n",
    "                    keypoints_block.append(cv2.KeyPoint(i, j, step))\n",
    "            \n",
    "            _, descriptors_block = SIFTdetector.compute(block,keypoints_block)   \n",
    "            descriptors.extend(descriptors_block)\n",
    "    \n",
    "    Test_descriptors.append(descriptors)\n",
    "    \n",
    "Test_descriptors = np.array(Test_descriptors)\n",
    "nsamples, nx, ny = Test_descriptors.shape\n",
    "Test_descriptors = Test_descriptors.reshape((nsamples, nx*ny))\n",
    "Test_descriptors_scaled = transformer.transform(Test_descriptors)\n",
    "Test_descriptors_scaled = Test_descriptors_scaled.reshape((nsamples, nx, ny))\n",
    "\n",
    "for image in range(len(test_images_filenames)):\n",
    "    words=codebook.predict(Test_descriptors_scaled[image])\n",
    "    visual_words_test[image,:]=np.bincount(words,minlength=k) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compute the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = 100*knn_best.score(visual_words_test, test_labels)\n",
    "print(accuracy)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dimensionality reduction, with PCA and LDA. Following the example just seen before, we can also do Cross-Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "pca = PCA(n_components=None)\n",
    "VWpca = pca.fit_transform(visual_words)\n",
    "\n",
    "# Find best parameters\n",
    "knnpca = KNeighborsClassifier(n_jobs=-1)\n",
    "clfpca = GridSearchCV(knnpca, parameters, cv=5) # StratifiedKFold n_splits = 5\n",
    "clfpca.fit(VWpca, train_labels) \n",
    "meanspca = 100 * clfpca.cv_results_['mean_test_score']\n",
    "print (\"Maximum accuracy for validation set is\", max(meanspca), \"with n_neighbors =\", clfpca.best_params_['n_neighbors'], \"and metric =\",  clfpca.best_params_['metric']) \n",
    "\n",
    "# Train again with best parameters\n",
    "knnpca_best = KNeighborsClassifier(n_neighbors= clfpca.best_params_['n_neighbors'],n_jobs=-1,metric=clfpca.best_params_['metric'])\n",
    "knnpca_best.fit(VWpca, train_labels) \n",
    "\n",
    "vwtestpca = pca.transform(visual_words_test)\n",
    "accuracy = 100*knnpca_best.score(vwtestpca, test_labels)\n",
    "print(accuracy)\n",
    "\n",
    "\n",
    "# Linear Discriminant Analysis\n",
    "lda = LinearDiscriminantAnalysis(n_components=None)\n",
    "VWlda = lda.fit_transform(visual_words,train_labels)\n",
    "\n",
    "# Find best parameters\n",
    "knnlda = KNeighborsClassifier(n_jobs=-1)\n",
    "clflda = GridSearchCV(knnlda, parameters, cv=5) # StratifiedKFold n_splits = 5\n",
    "clflda.fit(VWlda, train_labels) \n",
    "meanslda = 100 * clflda.cv_results_['mean_test_score']\n",
    "print (\"Maximum accuracy for validation set is\", max(meanslda), \"with n_neighbors =\", clflda.best_params_['n_neighbors'], \"and metric =\",  clflda.best_params_['metric']) \n",
    "\n",
    "# Train again with best parameters\n",
    "knnlda_best = KNeighborsClassifier(n_neighbors= clflda.best_params_['n_neighbors'],n_jobs=-1,metric=clflda.best_params_['metric'])\n",
    "knnlda_best.fit(VWlda, train_labels) \n",
    "\n",
    "vwtestlda = lda.transform(visual_words_test)\n",
    "accuracy = 100*knnlda_best.score(vwtestlda, test_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**RESULTS AND DISCUSSION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We first implemented **Dense SIFT**. Compared to last weeks's results, \n",
    "we improved around 10-15%, depending on dimensionality reduction and other factors.\n",
    "Then, we analysed the behaviour of Dense SIFT when changing the number of pixels between keypoints.\n",
    "The results can be seen in the following graphic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "step = [4,6,8,10,12,14,16,18,20]\n",
    "accuracy = [70.63,74.10,75.09,76.08,76.08,77.32,77.69,75.58,76.70]\n",
    "lda_accuracy = [83.39,84.13,83.27,81.16,83.27,81.04,81.9,80.79,80.17]\n",
    "pca_accuracy = [72.49,73.60,74.34,77.32,77.69,76.08,76.70,75.24,77.19]\n",
    "\n",
    "# Accuracy\n",
    "plt.scatter(step, accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Step size\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "ymax = max(accuracy)\n",
    "xpos = accuracy.index(ymax)\n",
    "xmax = step[xpos]\n",
    "plt.annotate('   local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% accuracy at step=\",xmax)\n",
    "    \n",
    "\n",
    "# Principal Component Analysis\n",
    "plt.scatter(step, pca_accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Step size\")\n",
    "plt.ylabel(\"PCA Accuracy (%)\")\n",
    "ymax = max(pca_accuracy)\n",
    "xpos = pca_accuracy.index(ymax)\n",
    "xmax = step[xpos]\n",
    "plt.annotate('   local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% PCA accuracy at step=\",xmax)\n",
    "\n",
    "    \n",
    "# Linear Discriminant Analysis\n",
    "plt.scatter(step, lda_accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Step size\")\n",
    "plt.ylabel(\"LDA Accuracy (%)\")\n",
    "ymax = max(lda_accuracy)\n",
    "xpos = lda_accuracy.index(ymax)\n",
    "xmax = step[xpos]\n",
    "plt.annotate('    local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% LDA accuracy at step=\",xmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We have decided that a step of 8 pixels is the best one, as it accomplishes\n",
    "one of the best LDA accuracies while not taking much time to complete the computations.\n",
    "\n",
    "After that, we resized the images to **new scales** to see if the performance \n",
    "could be further improved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scale = [100,90,80,75,70,65,55,50,25]\n",
    "accuracy = [75.09,74.47,74.72,75.83,75.09,73.60,72.86,71.49,55.88]\n",
    "lda_accuracy = [83.27,81.53,72.42,81.53,81.53,80.66,77.57,77.32,60.47]\n",
    "pca_accuracy = [74.34,74.71,74.10,74.84,75.71,72.24,72.49,70.38,56.50]\n",
    "\n",
    "# Accuracy\n",
    "plt.scatter(scale, accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Scale (% of original)\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "ymax = max(accuracy)\n",
    "xpos = accuracy.index(ymax)\n",
    "xmax = scale[xpos]\n",
    "plt.annotate('   local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% accuracy at scale=\",xmax)\n",
    "\n",
    "    \n",
    "# Principal Component Analysis\n",
    "plt.scatter(scale, pca_accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Scale (% of original)\")\n",
    "plt.ylabel(\"PCA Accuracy (%)\")\n",
    "ymax = max(pca_accuracy)\n",
    "xpos = pca_accuracy.index(ymax)\n",
    "xmax = scale[xpos]\n",
    "plt.annotate('   local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% PCA accuracy at scale=\",xmax)\n",
    "    \n",
    "    \n",
    "# Linear Discriminant Analysis\n",
    "plt.scatter(scale, lda_accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Scale (% of original)\")\n",
    "plt.ylabel(\"LDA Accuracy (%)\")\n",
    "ymax = max(lda_accuracy)\n",
    "xpos = lda_accuracy.index(ymax)\n",
    "xmax = scale[xpos]\n",
    "plt.annotate('    local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% LDA accuracy at scale=\",xmax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As it can be seen from before, the best results are accomplished when the image\n",
    "is not resized (we take into account the highest accuracy, LDA at scale 100%).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement different levels of **spatial pyramids**. Each level represents one subdivision of the image and for each division, we calculate dense SIFT descriptors. Then, we concatenate the descriptors of each block image and we use KNeighborsClassifier. We evaluate spatial pyramids using different dense SIFT steps and using **Cross-Validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dense sift STEP = 10\n",
    "level = [1, 2, 3, 4]\n",
    "accuracy = [76.08, 74.1, 69.27, 67.16]\n",
    "pca_accuracy = [77.69, 76.46, 70.01, 66.54]\n",
    "lda_accuracy = [81.16, 82.16, 75.84, 74.47]\n",
    "\n",
    "# Accuracy\n",
    "plt.scatter(level, accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Level \")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "ymax = max(accuracy)\n",
    "xpos = accuracy.index(ymax)\n",
    "xmax = level[xpos]\n",
    "plt.annotate('   local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% accuracy at level=\",xmax, \"with Dense SIFT step = 10\")\n",
    "\n",
    "    \n",
    "# Principal Component Analysis\n",
    "plt.scatter(level, pca_accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Level \")\n",
    "plt.ylabel(\"PCA Accuracy (%)\")\n",
    "ymax = max(pca_accuracy)\n",
    "xpos = pca_accuracy.index(ymax)\n",
    "xmax = level[xpos]\n",
    "plt.annotate('   local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% PCA accuracy at level=\",xmax, \"with Dense SIFT step = 10\")\n",
    "    \n",
    "    \n",
    "# Linear Discriminant Analysis\n",
    "plt.scatter(level, lda_accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Level\")\n",
    "plt.ylabel(\"LDA Accuracy (%)\")\n",
    "ymax = max(lda_accuracy)\n",
    "xpos = lda_accuracy.index(ymax)\n",
    "xmax = level[xpos]\n",
    "plt.annotate('    local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% LDA accuracy at level=\",xmax, \"with Dense SIFT step = 10\")\n",
    "\n",
    "# Dense sift STEP = 8\n",
    "accuracy = [75.09, 73.8, 70.87, 65.79]\n",
    "pca_accuracy = [74.72, 71.87, 71.74, 66.91]\n",
    "lda_accuracy = [83.27, 81.28, 75.71, 70.38]\n",
    "\n",
    "# Accuracy\n",
    "plt.scatter(level, accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Level \")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "ymax = max(accuracy)\n",
    "xpos = accuracy.index(ymax)\n",
    "xmax = level[xpos]\n",
    "plt.annotate('   local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% accuracy at level=\",xmax, \"with Dense SIFT step = 8\")\n",
    "\n",
    "    \n",
    "# Principal Component Analysis\n",
    "plt.scatter(level, pca_accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Level \")\n",
    "plt.ylabel(\"PCA Accuracy (%)\")\n",
    "ymax = max(pca_accuracy)\n",
    "xpos = pca_accuracy.index(ymax)\n",
    "xmax = level[xpos]\n",
    "plt.annotate('   local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% PCA accuracy at level=\",xmax, \"with Dense SIFT step = 8\")\n",
    "    \n",
    "    \n",
    "# Linear Discriminant Analysis\n",
    "plt.scatter(level, lda_accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Level\")\n",
    "plt.ylabel(\"LDA Accuracy (%)\")\n",
    "ymax = max(lda_accuracy)\n",
    "xpos = lda_accuracy.index(ymax)\n",
    "xmax = level[xpos]\n",
    "plt.annotate('    local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% LDA accuracy at level=\",xmax, \"with Dense SIFT step = 8\")\n",
    "\n",
    "# Dense sift STEP = 8, using cross-validatiion\n",
    "accuracy = [79.43, 78.56, 75.09, 70.75]\n",
    "\n",
    "# Accuracy\n",
    "plt.scatter(level, accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Level \")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "ymax = max(accuracy)\n",
    "xpos = accuracy.index(ymax)\n",
    "xmax = level[xpos]\n",
    "plt.annotate('   local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% accuracy at level=\",xmax, \"with Dense SIFT step = 8 and cross-validation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observe, spatial pyramids for Dense SIFT descriptors and KNeighborsClassifier do not help too much in the improvement of the accuracy. The majority of experiments show that level 1 is the best level (no division of the image).\n",
    "\n",
    "Following the spatial pyramids, we implemented several methodologies for data normalization: L1-norm, L2-norm, MinMaxAbs norm and max norm. Due to a bug in the implementation in Scikit-Learn's library, power-norm could not be implemented.  \n",
    "\n",
    "\n",
    "With normalisation, we also checked the difference between using or not using Cross-Validation to select the best parameters for the K-NN algorithm. The first 3 graphics show the accuracies without Cross-Validation, using the default parameters from before. The last 3 graphics show the accuracies with Cross-Validation, using the best parameters found by the Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "norms = [\"L1\",\"L2\",\"Max\",\"MinMax\"]\n",
    "accuracy = [78.93,74.47,73.72,76.20]\n",
    "pca_accuracy = [74.47,75.96,73.72,72.72]\n",
    "lda_accuracy = [82.89,81.53,83.14,83.14]\n",
    "\n",
    "# NO CROSS-VALIDATION \n",
    "# Accuracy\n",
    "plt.scatter(norms, accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Normalization Method \")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "ymax = max(accuracy)\n",
    "xpos = accuracy.index(ymax)\n",
    "xmax = norms[xpos]\n",
    "plt.annotate('   local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% accuracy with=\",xmax)\n",
    "\n",
    "    \n",
    "# Principal Component Analysis\n",
    "plt.scatter(norms, pca_accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Normalization Method \")\n",
    "plt.ylabel(\"PCA Accuracy (%)\")\n",
    "ymax = max(pca_accuracy)\n",
    "xpos = pca_accuracy.index(ymax)\n",
    "xmax = norms[xpos]\n",
    "plt.annotate('   local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% PCA accuracy with=\",xmax)\n",
    "    \n",
    "    \n",
    "# Linear Discriminant Analysis\n",
    "plt.scatter(norms, lda_accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Normalization Method \")\n",
    "plt.ylabel(\"LDA Accuracy (%)\")\n",
    "ymax = max(lda_accuracy)\n",
    "xpos = lda_accuracy.index(ymax)\n",
    "xmax = norms[xpos]\n",
    "plt.annotate('    local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% LDA accuracy with=\",xmax)\n",
    "\n",
    "\n",
    "# WITH CROSS-VALIDATION\n",
    "accuracy = [79.30,78.93,78.68,79.42]\n",
    "pca_accuracy = [75.21,75.96,72.86,75.71]\n",
    "lda_accuracy = [82.89,82.40,81.78,81.41]\n",
    "\n",
    "# Accuracy\n",
    "plt.scatter(norms, accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Normalization Method \")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "ymax = max(accuracy)\n",
    "xpos = accuracy.index(ymax)\n",
    "xmax = norms[xpos]\n",
    "plt.annotate('   local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% accuracy with=\",xmax)\n",
    "\n",
    "    \n",
    "# Principal Component Analysis\n",
    "plt.scatter(norms, pca_accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Normalization Method \")\n",
    "plt.ylabel(\"PCA Accuracy (%)\")\n",
    "ymax = max(pca_accuracy)\n",
    "xpos = pca_accuracy.index(ymax)\n",
    "xmax = norms[xpos]\n",
    "plt.annotate('   local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% PCA accuracy with=\",xmax)\n",
    "    \n",
    "    \n",
    "# Linear Discriminant Analysis\n",
    "plt.scatter(norms, lda_accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Normalization Method \")\n",
    "plt.ylabel(\"LDA Accuracy (%)\")\n",
    "ymax = max(lda_accuracy)\n",
    "xpos = lda_accuracy.index(ymax)\n",
    "xmax = norms[xpos]\n",
    "plt.annotate('    local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% LDA accuracy with=\",xmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plots, Cross-Validation helps improve the results in the Accuracy (without dimensionality reduction). When LDA or PCA is applied to the data, improvements when using Cross-Validation can only be seen with L1 and L2 normalization (with Max and MinMax, sometimes it improves and sometimes not).\n",
    "\n",
    "Regarding the normalization process, we can see from the first 3 plots that the results are mixed when compared to the first results presented. The normal accuracy again is improved when doing data normalization, but when performing LDA and PCA the normalization does not improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we implement **SVM classifier** using sklearn and different kernel functions. We first apply standardScalar function. This function standardize features by removing the mean and scaling to unit variance. We use a linear kernel, a RBF kernel and our own histogram intersection kernel function (histogramIntersection(M,N)). The results we got are the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "kernel = [\"Linear\", \"RBF\", \"Histogram Intersection Kernel\"]\n",
    "accuracy = [77.69, 80.17, 81.04]\n",
    "\n",
    "# Accuracy\n",
    "plt.scatter(kernel, accuracy)\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel(\"Kernel \")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "ymax = max(accuracy)\n",
    "xpos = accuracy.index(ymax)\n",
    "xmax = kernel[xpos]\n",
    "plt.annotate('   local max', xy=(xmax, ymax), xytext=(xmax, ymax+5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),)\n",
    "plt.show()\n",
    "print(\"Local max is\", ymax,\"% accuracy with kernel:\",xmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the best accuracy is obtained with Histogram Intersection Kernel. We also observe that there is not a signicant improvement in tha accuracy using SVM with respecto to other classifiers. NOTE: We have not considered Cross-Validation results in SVM because it took a long time to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIONAL - Fisher Vectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the fishervector library, developed by Jonas Rothfuss and Fabio Ferreira. More info: https://github.com/jonasrothfuss/fishervector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "from sklearn import svm\n",
    "import sys, glob\n",
    "\n",
    "run_optional = True\n",
    "\n",
    "if run_optional:\n",
    "    def dictionary(descriptors, N):        \n",
    "        em = cv2.ml.EM_create()\n",
    "        em.setClustersNumber(N)\n",
    "        em.trainEM(descriptors)\n",
    "\n",
    "        return np.float32(em.getMeans()), \\\n",
    "            np.float32(em.getCovs()), np.float32(em.getWeights())[0]\n",
    "\n",
    "    def image_descriptors(file):\n",
    "        img = cv2.imread(file, 0)\n",
    "        img = cv2.resize(img, (256, 256))\n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        _ , descriptors = sift.detectAndCompute(img, None)\n",
    "        return descriptors\n",
    "\n",
    "    def folder_descriptors(folder):\n",
    "        files = glob.glob(folder + \"/*.jpg\")\n",
    "        print(\"Calculating descriptors. Number of images is\", len(files))\n",
    "        return np.concatenate([image_descriptors(file) for file in files])\n",
    "\n",
    "\n",
    "    def likelihood_moment(x, ytk, moment):\n",
    "        x_moment = np.power(np.float32(x), moment) if moment > 0 else np.float32([1])\n",
    "        return x_moment * ytk\n",
    "\n",
    "    def likelihood_statistics(samples, means, covs, weights):\n",
    "        gaussians, s0, s1,s2 = {}, {}, {}, {}\n",
    "        samples = zip(range(0, len(samples)), samples)\n",
    "\n",
    "        g = [multivariate_normal(mean=means[k], cov=covs[k]) for k in range(0, len(weights)) ]\n",
    "        for index, x in samples:\n",
    "            gaussians[index] = np.array([g_k.pdf(x) for g_k in g])\n",
    "\n",
    "        for k in range(0, len(weights)):\n",
    "            s0[k], s1[k], s2[k] = 0, 0, 0\n",
    "            for index, x in samples:\n",
    "                probabilities = np.multiply(gaussians[index], weights)\n",
    "                probabilities = probabilities / np.sum(probabilities)\n",
    "                s0[k] = s0[k] + likelihood_moment(x, probabilities[k], 0)\n",
    "                s1[k] = s1[k] + likelihood_moment(x, probabilities[k], 1)\n",
    "                s2[k] = s2[k] + likelihood_moment(x, probabilities[k], 2)\n",
    "\n",
    "        return s0, s1, s2\n",
    "\n",
    "    def fisher_vector_weights(s0, s1, s2, means, covs, w, T):\n",
    "        return np.float32([((s0[k] - T * w[k]) / np.sqrt(w[k]) ) for k in range(0, len(w))])\n",
    "\n",
    "    def fisher_vector_means(s0, s1, s2, means, sigma, w, T):\n",
    "        return np.float32([(s1[k] - means[k] * s0[k]) / (np.sqrt(w[k] * sigma[k])) for k in range(0, len(w))])\n",
    "\n",
    "    def fisher_vector_sigma(s0, s1, s2, means, sigma, w, T):\n",
    "        return np.float32([(s2[k] - 2 * means[k]*s1[k]  + (means[k]*means[k] - sigma[k]) * s0[k]) / (np.sqrt(2*w[k])*sigma[k])  for k in range(0, len(w))])\n",
    "\n",
    "    def normalize(fisher_vector):\n",
    "        v = np.sqrt(abs(fisher_vector)) * np.sign(fisher_vector)\n",
    "        return v / np.sqrt(np.dot(v, v))\n",
    "\n",
    "    def fisher_vector(samples, means, covs, w):\n",
    "        s0, s1, s2 =  likelihood_statistics(samples, means, covs, w)\n",
    "        T = samples.shape[0]\n",
    "        covs = np.float32([np.diagonal(covs[k]) for k in range(0, covs.shape[0])])\n",
    "        a = fisher_vector_weights(s0, s1, s2, means, covs, w, T)\n",
    "        b = fisher_vector_means(s0, s1, s2, means, covs, w, T)\n",
    "        c = fisher_vector_sigma(s0, s1, s2, means, covs, w, T)\n",
    "        fv = np.concatenate([np.concatenate(a), np.concatenate(b), np.concatenate(c)])\n",
    "        fv = normalize(fv)\n",
    "        return fv\n",
    "\n",
    "    def generate_gmm(input_folder, N):\n",
    "        words = np.concatenate([folder_descriptors(folder) for folder in glob.glob(input_folder + '/*')]) \n",
    "        print(\"Training GMM of size\", N)\n",
    "        means, covs, weights = dictionary(words, N)\n",
    "        #Throw away gaussians with weights that are too small:\n",
    "        th = 1.0 / N\n",
    "        means = np.float32([m for k,m in zip(range(0, len(weights)), means) if weights[k] > th])\n",
    "        covs = np.float32([m for k,m in zip(range(0, len(weights)), covs) if weights[k] > th])\n",
    "        weights = np.float32([m for k,m in zip(range(0, len(weights)), weights) if weights[k] > th])\n",
    "\n",
    "        np.save(\"means.gmm\", means)\n",
    "        np.save(\"covs.gmm\", covs)\n",
    "        np.save(\"weights.gmm\", weights)\n",
    "        return means, covs, weights\n",
    "\n",
    "    def get_fisher_vectors_from_folder(folder, gmm):\n",
    "        files = glob.glob(folder + \"/*.jpg\")\n",
    "        return np.float32([fisher_vector(image_descriptors(file), *gmm) for file in files])\n",
    "\n",
    "    def fisher_features(folder, gmm):\n",
    "        folders = glob.glob(folder + \"/*\")\n",
    "        features = {f : get_fisher_vectors_from_folder(f, gmm) for f in folders}\n",
    "        return features\n",
    "\n",
    "    def train(gmm, features):\n",
    "        X = np.concatenate(features.values())\n",
    "        Y = np.concatenate([np.float32([i]*len(v)) for i,v in zip(range(0, len(features)), features.values())])\n",
    "\n",
    "        clf = svm.SVC()\n",
    "        clf.fit(X, Y)\n",
    "        return clf\n",
    "\n",
    "    def success_rate(classifier, features):\n",
    "        print(\"Applying the classifier...\")\n",
    "        X = np.concatenate(np.array(features.values()))\n",
    "        Y = np.concatenate([np.float32([i]*len(v)) for i,v in zip(range(0, len(features)), features.values())])\n",
    "        res = float(sum([a==b for a,b in zip(classifier.predict(X), Y)])) / len(Y)\n",
    "        return res\n",
    "    \n",
    "    def load_gmm(folder = \".\"):\n",
    "        files = [\"means.gmm.npy\", \"covs.gmm.npy\", \"weights.gmm.npy\"]\n",
    "        return map(lambda file: load(file), map(lambda s : folder + \"/\" , files))\n",
    "\n",
    "working_folder = \"../../Databases/MIT_split/train/\"\n",
    "gmm = generate_gmm(working_folder, 10)\n",
    "fisher_features_train = fisher_features(working_folder, gmm)\n",
    "#TBD, split the features into training and validation\n",
    "classifier = train(gmm, fisher_features_train)\n",
    "rate = success_rate(classifier, fisher_features_train)\n",
    "print(\"Success rate is\", rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, this fisher vectors function does not return a good accuracy. We tried a different method: version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FISHER VERSION 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import cPickle as pickle\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn import preprocessing\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm \n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# LOAD FUNCTIONS FOR GMM AND FISHER VECTOR\n",
    "def fisher_vector(xx, gmm):\n",
    "    \"\"\"Computes the Fisher vector on a set of descriptors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    xx: array_like, shape (N, D) or (D, )\n",
    "        The set of descriptors\n",
    "    gmm: instance of sklearn mixture.GMM object\n",
    "        Gauassian mixture model of the descriptors.\n",
    "    Returns\n",
    "    -------\n",
    "    fv: array_like, shape (K + 2 * D * K, )\n",
    "        Fisher vector (derivatives with respect to the mixing weights, means\n",
    "        and variances) of the given descriptors.\n",
    "    Reference\n",
    "    ---------\n",
    "    J. Krapac, J. Verbeek, F. Jurie.  Modeling Spatial Layout with Fisher\n",
    "    Vectors for Image Categorization.  In ICCV, 2011.\n",
    "    http://hal.inria.fr/docs/00/61/94/03/PDF/final.r1.pdf\n",
    "    \"\"\"\n",
    "    xx = np.atleast_2d(xx)\n",
    "    N = xx.shape[0]\n",
    "\n",
    "    # Compute posterior probabilities.\n",
    "    Q = gmm.predict_proba(xx)  # NxK\n",
    "\n",
    "    # Compute the sufficient statistics of descriptors.\n",
    "    Q_sum = np.sum(Q, 0)[:, np.newaxis] / N\n",
    "    Q_xx = np.dot(Q.T, xx) / N\n",
    "    Q_xx_2 = np.dot(Q.T, xx ** 2) / N\n",
    "\n",
    "    # Compute derivatives with respect to mixing weights, means and variances.\n",
    "    d_pi = Q_sum.squeeze() - gmm.weights_\n",
    "    d_mu = Q_xx - Q_sum * gmm.means_\n",
    "    d_sigma = (\n",
    "        - Q_xx_2\n",
    "        - Q_sum * gmm.means_ ** 2\n",
    "        + Q_sum * gmm.covariances_\n",
    "        + 2 * Q_xx * gmm.means_)\n",
    "\n",
    "    # Merge derivatives into a vector.\n",
    "    #return np.hstack((d_pi, d_mu.flatten(), d_sigma.flatten()))\n",
    "    return np.hstack((d_mu.flatten(), d_sigma.flatten()))\n",
    "\n",
    "def power_normalize(xx, alpha):\n",
    "    \"\"\" Computes a alpha-power normalization for the matrix xx. \"\"\"\n",
    "    return np.sign(xx) * np.abs(xx) ** alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# Original on Raw PHOCs with PCA\n",
    "gmm = GMM(n_components = 64, covariance_type = 'diag')\n",
    "# check what to fit in GMM\n",
    "\n",
    "gmm.fit(X_train_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# TRAINING TIME\n",
    "train_fisher_vectors = []\n",
    "\n",
    "for i in range (len(X_train_minmax)):\n",
    "    train_fisher\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, fisher vectors function could not be completed because there were many memory errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running all these different experiments, we summarize the main results we got using these techniques to improve our classifier:\n",
    "1. Dense SIFT with independece between step and scale improves the classifier with respect to SIFT keypoint detector in more than 15 % of accuracy (Test set) \n",
    "2. Data normalization slightly improves normal accuracy, but it does not change accuracy when we perform dimensionality reduction LDA or PCA. One possible reason is that descriptors have similar number of components as we are using Dense SIFT with the same step and scale for all the images.\n",
    "3. Cross-Validation has been used to find the best parameters in the Knn classifier and SVM classifier. It helps to find the optimal parameters of the classifier, but the improvement in accuracy (Test set) is limited.\n",
    "4. Spatial pyramids have not helped us to improve the accuracy of the classifier.\n",
    "5. SVM classifier accuracy improves when we use our histogram intersection kernel but it is very slow. We could not use Cross-Validation for SVM because it takes too much time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}